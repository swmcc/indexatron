# ğŸ¤– Indexatron

> *Teaching machines to understand family memories*

An experimental Python service that uses local LLMs via Ollama to analyze family photos. Part of a larger experiment to enable semantic search across decades of family memories.

## ğŸ§ª Experiment Status

This is a **science experiment** - proving that local AI can meaningfully analyze family photos before integrating with a production system.

| Branch | Status | What it proves |
|--------|--------|----------------|
| `01-project-setup` | âœ… | Project structure works |
| `02-ollama-connection` | âœ… | Can talk to Ollama |
| `03-image-analysis` | âœ… | LLaVA understands photos |
| `04-embedding-generation` | âœ… | Can generate embeddings |
| `05-batch-processing` | âœ… | Can process many photos |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Indexatron                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Image     â”‚â”€â”€â”€â–¶â”‚   LLaVA     â”‚â”€â”€â”€â–¶â”‚  Analysis   â”‚ â”‚
â”‚  â”‚   Input     â”‚    â”‚   (7B)      â”‚    â”‚   JSON      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                            â”‚
â”‚                            â–¼                            â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                     â”‚   nomic-    â”‚                     â”‚
â”‚                     â”‚ embed-text  â”‚                     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                            â”‚                            â”‚
â”‚                            â–¼                            â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                     â”‚  768-dim    â”‚                     â”‚
â”‚                     â”‚  Embedding  â”‚                     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Results JSON   â”‚
                    â”‚  (for now)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

### Python 3.11+

```bash
python --version  # Should be 3.11 or higher
```

### Ollama

```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service (runs in background)
ollama serve

# Or run as a service
brew services start ollama
```

### Required Models

```bash
# Pull the vision model (~4.7GB)
ollama pull llava:7b

# Pull the embedding model (~274MB)
ollama pull nomic-embed-text

# Verify models are installed
ollama list
```

Expected output:
```
NAME                       SIZE
llava:7b                   4.7 GB
nomic-embed-text:latest    274 MB
```

## ğŸš€ Installation

```bash
# Clone the repo
git clone <repo-url>
cd indexatron

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“ Project Structure

```
indexatron/
â”œâ”€â”€ README.md                 # You are here
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pyproject.toml           # Modern packaging
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/indexatron/          # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py            # Ollama client wrapper
â”‚   â”œâ”€â”€ analyzer.py          # LLaVA image analysis
â”‚   â”œâ”€â”€ embedder.py          # Embedding generation
â”‚   â”œâ”€â”€ processor.py         # Batch processing
â”‚   â””â”€â”€ models.py            # Pydantic models
â”‚
â”œâ”€â”€ scripts/                 # CLI scripts
â”‚   â”œâ”€â”€ test_connection.py   # Verify Ollama works
â”‚   â”œâ”€â”€ analyze_single.py    # Analyze one image
â”‚   â”œâ”€â”€ generate_embedding.py
â”‚   â””â”€â”€ process_batch.py     # Process all images
â”‚
â”œâ”€â”€ test_images/             # Sample images (git tracked)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ results/                 # Output (gitignored)
    â””â”€â”€ ...
```

## ğŸ”¬ Usage

### Test Ollama Connection

```bash
python scripts/test_connection.py
```

### Analyze a Single Image

```bash
python scripts/analyze_single.py test_images/photo.jpg
# Output: results/analysis_photo.json
```

### Generate Embedding

```bash
python scripts/generate_embedding.py test_images/photo.jpg
# Output: results/embedding_photo.json
```

### Batch Process All Images

```bash
python scripts/process_batch.py
# Output: results/batch_results.json
```

## ğŸ“Š Output Format

### Analysis JSON

```json
{
  "description": "A family gathering at the beach during sunset...",
  "location": {
    "setting": "beach",
    "type": "outdoor"
  },
  "people": [
    {"description": "young boy, approximately 8 years old", "position": "center"},
    {"description": "woman, approximately 35 years old", "position": "left"}
  ],
  "categories": ["family", "outdoor", "beach", "sunset"],
  "era": {
    "decade": "1990s",
    "confidence": "medium",
    "reasoning": "Photo quality and clothing style suggest mid-90s"
  },
  "mood": "warm, nostalgic, joyful",
  "colors": ["orange", "blue", "golden"],
  "objects": ["beach towel", "sandcastle", "cooler"]
}
```

### Embedding JSON

```json
{
  "filename": "photo.jpg",
  "model": "nomic-embed-text",
  "dimensions": 768,
  "embedding": [0.123, -0.456, 0.789, ...]
}
```

## ğŸ§  The Models

### LLaVA 7B (Vision)

- **Purpose**: Understand image content
- **Size**: ~4.7GB
- **Strengths**: Good at describing scenes, identifying objects, reading text
- **Limitations**: May hallucinate details, era estimation is approximate

### nomic-embed-text (Embeddings)

- **Purpose**: Convert descriptions to searchable vectors
- **Size**: ~274MB
- **Output**: 768-dimensional vectors
- **Use case**: Finding similar photos via cosine similarity

## ğŸ”— Related

This is part of a larger experiment:

- **Rails API**: Provides photo storage and similarity search endpoints
- **Indexatron** (this): Analyzes photos and generates embeddings
- **Future**: UI for browsing results and finding similar photos

## ğŸ“ Experiment Log

### 2026-02-22

- Initial project setup
- Testing LLaVA on family photos
- Generating first embeddings

---

*ğŸ¤– Built with curiosity and local LLMs*
